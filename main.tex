%%%% kr-instructions.tex -- version 1.3 (11-Jan-2021)

\typeout{KR2026 Instructions for Authors}

% These are the instructions for authors for KR-26.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

\usepackage{kr}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,calc,shapes.misc}

\newcommand\algorithmicprocedure{\textbf{procedure}}
\newcommand{\algorithmicendprocedure}{\algorithmicend\ \algorithmicprocedure}
\makeatletter
\newcommand\PROCEDURE[3][default]{%
  \ALC@it
  \algorithmicprocedure\ \textsc{#2}(#3)%
  \ALC@com{#1}%
  \begin{ALC@prc}%
}
\newcommand\ENDPROCEDURE{%
  \end{ALC@prc}%
  \ifthenelse{\boolean{ALC@noend}}{}{%
    \ALC@it\algorithmicendprocedure
  }%
}
\newcommand\CALL[2]{\textsc{#1}(#2)}
\newenvironment{ALC@prc}{\begin{ALC@g}}{\end{ALC@g}}
\makeatother

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.
%PDF Info Is REQUIRED.
\pdfinfo{
/TemplateVersion (KR.2026.0)
}

\title{Current Limitations of LLM Guided Program Synthesis}
\input{commands}

% Single author syntax
\iffalse % (remove the multiple-author syntax below and \iffalse ... \fi here)
\author{%
    Author name
    \affiliations
    Affiliation
    \emails
    email@example.com    % email
}
\fi
% Multiple author syntax
% \author{%
% First Author$^1$\and
% Second Author$^2$\and
% Third Author$^{2,3}$\and
% Fourth Author$^4$ \\
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation \\
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
% }

\author{Anonymous Author(s)}

\begin{document}

\maketitle

\begin{abstract}

Large Language Models (LLMs) exhibit strong performance for program synthesis tasks including text-to-SQL, up to 86.6\% accuracy on the Spider 1.0 benchmark. The top-performing methods bridge the semantic gap between natural language and SQL by using prompt engineering, fine-tuning, or in-context learning techniques; however, these purely neural techniques remain prone to hallucinations.
Existing symbolic approaches to improve accuracy on program synthesis tasks generally rely on post-hoc verification, which fails to constrain unproductive path exploration, or constrained decoding techniques that rely on the underlying distribution of the model.
The effectiveness of symbolic guidance under complex queries and the semantic ambiguity inherent in text-to-SQL benchmarks, however, remains unclear.
In this work, we study the \emph{``ground truth''} errors made by LLMs in the text-to-SQL task looking at both fine-tuned models and a general-purpose model optimized for code generation.
We study the incremental prediction errors LLMs make and find that errors can be grouped into six error categories, three of which are amenable to simple symbolic fixes.
Importantly, we find that for some error types, correct predictions do not rank near the top of the LLMs likely predictions, implying that \emph{constrained decoding alone is insufficient} to guide synthesis on hard problems.
Our findings provide interpretable insight into where LLMs fail and suggest promising, neuro-symbolic directions for improving LLM guided program synthesis.

% Large Language Models (LLMs) have exhibited strong performance on program synthesis tasks such as text-to-SQL, achieving up to $\sim$87\% accuracy on state-of-the-art challenges such as the Yale Semantic Parsing (Spider). However, most of the top-performing methods bridge the semantic gap between natural language and SQL using clever prompt engineering, fine-tuning, or in-context learning techniques.
% All these methods rely on the LLM to generate the entire SQL query and then perform a post-hoc verification step to check the validity of the generated query. Logically, this approach does not constrain the LLM from generating paths that lead to ``semantic deadends''. Further, constrained decoding helps alleviate this issue by checking if it can find a valid continuation within the program space of the next predicted token. However, the effectiveness of this approach under complex queries and semantic ambiguity such as in SQL is questionable. To this end, we observe that the so called \emph{``ground truth'' mistakes that LLMs make using both a fine-tuned and prompt-engineered model with constrained decoding prove insufficient} to address \wip{<z>} of these categories.
% Our findings provide interpretable insight into where LLMs fail and suggest promising, neuro-symbolic directions for improving LLM guided program synthesis.

\end{abstract}

% \begin{figure*}
%     \centering
%     \input{figures/overview.tikz}
%     \caption{The overall system architecture. The Language Model Generator is prompted to generate a SQL query given a database schema and natural language question. The generator returns a list of lexemes to the Validator which incrementally validates the most probable lexeme against the gold one and corrects the generator if there is a mismatch.}
%     \label{fig:overview}
% \end{figure*}

\input{sections/introduction}

\input{sections/related_work}

% \section{TSQL}
% \input{sections/syntax}

% \input{sections/semantics}

\input{sections/approach}

\input{sections/experimentalSetup}

\input{sections/taxonomy}

\input{sections/results.tex}

\input{sections/conclusion}


%% The file kr.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{kr}
\bibliography{bibliography}

\end{document}
