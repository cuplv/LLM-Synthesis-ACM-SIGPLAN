\section{Experimental Setup}

\subsection{Implementation}
Our evaluation is carried out in Python and leverages open-source LLMs hosted on HuggingFace for text-to-SQL generation, including XiYanSQL-QwenCoder-7B-2504, deepseek-coder-6.7b-instruct, and XiYanSQL-QwenCoder-14B-2504. Models are loaded in 4-bit quantized mode when possible to reduce GPU memory usage, and generation is performed on A100 NVIDIA GPUs with 40GB VRAM for XiYanSQL-QwenCoder-7B-2504, deepseek-coder-6.7b-instruct and 80GB for XiYanSQL-QwenCoder-14B-2504 with automatic device mapping and offloading for larger models.

\subsection{Model Choice}
Finetuned models XiYanSQL-QwenCoder-7B-2504 and XiYanSQL-QwenCoder-14B-2504 were chosen due to their high performance on the Spider dataset ~\cite{xiYan}. We also use the general purpose deepseek-coder-6.7b-instruct model to observe the effect fine-tuning has on the kinds of mistakes LLMs make.

\subsection{Dataset}
We evaluated 369 hard and extra hard queries from the test split of the Spider 1.0 dataset ~\cite{spider-sql}.
The hard and extra hard queries were chosen to surface errors that arise from complex queries, the queries that LLMs tend to have trouble synthesizing.
We excluded the queries whose prompts, which included the schemas for the relevant database, did not fit into the GPU memory.

\subsection{Schema Selection}
For each database, we extract the corresponding schema definition from the provided .sql file and include its contents verbatim in the model prompt. No additional preprocessing the of schema elements is performed. This allows us to isolate lexeme-level generation errors without introducing confounding effects from schema filtering mechanisms.
% \begin{table}[]
% \begin{tabular}{rr}
% \multicolumn{1}{l}{}     & \multicolumn{1}{l}{Count} \\ \hline
% Total Number of Queries  & 30                        \\
% Total Number of Mistakes & 112                       \\
% Gold Lexeme Found in Top-k           & 70                        \\
% Gold Lexeme Not Found in Top-k       & 42
% \end{tabular}
% \begin{tabular}{l l}
% \hline
% Total Not Found errors & 41 \\
% Gold was TSQL keyword & 23 \\
% Gold was NOT TSQL keyword & 18 \\
% \hline
% \end{tabular}
% \caption{This table shows the empirical evaluation results of our LLM guided program synthesis approach. We tested against 30 queries. Across these 30 queries, the LLM made 112 mistakes. Of these mistakes, 70 of them were recoverable as they were generated by the LLM and stored in the top-k beam. Forty-two mistakes were not recoverable.}
% \label{eval1}
% \end{table}


% \begin{table}[]
% \begin{tabular}{l l}
% \hline
% Total Number of Queries & 30 \\
% Total Number of Mistakes & 200 \\
% Gold Lexeme Found in Top-k & 77 \\
% Gold Lexeme Not Found in Top-k & 123 \\
% \hline
% \end{tabular}
% \begin{tabular}{l l}
% \hline
% Total Not Found errors & 117 \\
% Gold was SQL keyword & 16 \\
% Gold was NOT SQL keyword & 101 \\
% \hline
% \end{tabular}

% \caption{This table shows the empirical evaluation results for SQL generation. Mostly schema and aggregation errors. For the finetuned model eglym/DR-TEXT2SQL-CodeLlama2-7B }
% \label{eval1}
% \end{table}

% \begin{table}[]
% \begin{tabular}{l l}
% \hline
% Total Number of Queries & 30 \\
% Total Number of Mistakes & 276 \\
% % Gold Lexeme Found in Top-k & 77 \\
% % Gold Lexeme Not Found in Top-k & 123 \\

% \hline
% \end{tabular}
% \begin{tabular}{l l}
% \hline
% Total Not Found errors & 234 \\
% Gold was SQL keyword & 51 \\
% Gold was NOT SQL keyword & 183 \\
% \hline
% \end{tabular}

% \caption{Results for Qwen2.5-Coder-7B }
% \label{eval1}
% \end{table}

% SFT model - XiYanSQL-QwenCoder-7B-2504

% \begin{table*}[]
% \begin{tabular}{rrl}
% \hline
% \multicolumn{1}{l}{Type of Error} & \multicolumn{1}{l}{Rough Distribution} & Suggested Symbolic Fix \\ \hline
% \begin{tabular}[c]{@{}r@{}}Schema Mismatch\\ (also join on wrong column \\ errors could also go here)\end{tabular} & 50-70\% of errors & \begin{tabular}[c]{@{}l@{}}top-k: schema based decoding\\ usually the highest ranked schema element is gold\\ not in top-k: have the LLM rank schema elements\end{tabular} \\ \hline
% Structural & 10-20\% & \begin{tabular}[c]{@{}l@{}}easy: end of a query determine if you need a LIMIT \\ medium: \\ hard: catch all\end{tabular} \\ \hline
% WHERE &  &  \\ \hline
% Aggregates &  &  \\ \hline
% Other &  &  \\ \hline
% \end{tabular}
% \caption{Error Types and suggested symbolic fixes }
% \end{table*}

% \begin{table*}[]
% \begin{tabular}{|l|l|l|l|}
% \hline
%  & \begin{tabular}[c]{@{}l@{}}XGenerationLab/\\ XiYanSQL-QwenCoder-14B-2504\end{tabular} & \begin{tabular}[c]{@{}l@{}}XGenerationLab/\\ XiYanSQL-QwenCoder-7B-2504\end{tabular} & \begin{tabular}[c]{@{}l@{}}eglym/\\ DR-TEXT2SQL-CodeLlama2-7B\end{tabular} \\ \hline
% Schema (includes join) & 214 & 211 & 206 \\ \hline
% Structural & 31 & 45 & 32 \\ \hline
% \begin{tabular}[c]{@{}l@{}}Where \\ (note these \\ numbers are inflated)\end{tabular} & 22 & 24 & 19 \\ \hline
% Aggregate & 20 & 21 & 11 \\ \hline
% Other & 30 & 17 & 17 \\ \hline
% Total Errors & 318 & 318 & 285 \\ \hline
% \end{tabular}
% \caption{Different SFT models and general error distributions over 51 queries.}
% \end{table*}