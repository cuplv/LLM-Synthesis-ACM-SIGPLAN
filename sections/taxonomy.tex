
\begin{table*}[t]
\centering
\begin{tabular}{l r r r}
\toprule
\textbf{Error Category} & \textbf{deepseek-coder-6.7b-instruct} & \textbf{XiYanSQL-7B}  & \textbf{XiYanSQL-14B}\\
\midrule
\textbf{Structural / Clause Ordering} & 1385 & 583 & 472\\
\quad Limit Errors & 245 & 235 & 243\\
\quad Other Structural Errors & 1140 & 348 & 229\\
\midrule
\textbf{Schema Grounding} & 938 & 1179 & 1255\\
\quad Value Errors & 26 & 58 & 107\\
\quad Other Schema Errors & 912 & 1121 & 1148\\
\midrule
\textbf{Operator Errors} & 1447 & 312 & 202\\
\quad Where and Having Errors & 658 & 272 & 179\\
\quad Other Operator Errors & 789 & 40 & 26\\
\midrule
\textbf{Aggregate Function Errors} & 214 & 205 & 190\\
\textbf{Alias Errors} & 34 & 45 & 864\\
\textbf{Other / Unclassified} & 419 & 415 & 235\\
\bottomrule
\textbf{Total Errors} & 4437 & 2739 & 3221\\
\end{tabular}
\caption{Lexeme-level error distribution across models.}
\label{tab:error_comparison}
\end{table*}


\section{Taxonomy}
In this section, we outline our error taxonomy.

\subsection{Schema-based Errors}

Schema-based errors are defined as errors in which the model fails to correctly reference the database schema. These errors typically involve either selecting an incorrect table or column name, hallucinating schema attributes, or choosing an inappropriate value from the database for a condition.

\subsubsection{Incorrect Naming}
These errors are defined as ones where the model generates a lexeme that does not correspond to the correct schema element. For example, the model might predict FIXME instead of the correct column FIXME.

Symbolic Fix: \textbf{Easy} To address the generation of hallucinated schema values, we propose a symbolic fix \textit{schema-based constrained decoding} in which the first valid schema element that appears in the top-k is chosen. If no valid element is initially predicted, one can force the model to select from the set of valid schema elements, often yielding the correct token. Based on empirical results presented in FIXME, the highest valid schema element is FIXME percentage of the time the correct one. Such a technique would fix FIXME number of errors.

\subsubsection{Incorrect Values}

These errors typically occur in \texttt{WHERE} conditions, where the model selects an incorrect literal value, even though the query structure and schema reference are correct. For instance, given the question:

\begin{quote}
\emph{``What are the addresses, towns, and county information for all customers who live in the United States?''}
\end{quote}

the gold query is:

\begin{verbatim}
SELECT address_line_1, town_city, county
FROM Customers
WHERE Country = 'USA';
\end{verbatim}

A model might predict \texttt{'United'} as the value, which reflects a plausible misunderstanding of the questionâ€™s phrasing. Correctly mapping natural language values to the database representation often requires explicitly grounding the question in the schema or the database contents.

Symbolic Fix: \textbf{Moderate} One approach is to align natural language mentions with schema values, using either string similarity or enumeration over the database domain. This allows the model to select or rank candidates that correspond to actual entries in the database, mitigating errors in value selection.



\subsubsection{Wrong Naming}
these are errors like wrong column or table name (the gold is present in the schema but the predicted candidate is not)

\textbf{Symbolic fix}: Easy- if valid schema element in top-k then you use a schema based constrained decoding to select (usually the gold is the top most). If there is no valid schema element, have the LLM pick from or rank valid schema elements

\subsubsection{Wrong Values}

For where conditions - the wrong value is selected for example - "What are the addresses, towns, and county information for all customers who live in the United States?" but the gold query is

\begin{verbatim}
   SELECT $address_line_1$ ,  $town_city$ ,
   county FROM Customers WHERE Country  =  'USA'
\end{verbatim}

The LLM predicts \textbf{'United } which makes sense given the phrasing of the question but involves parsing out all the values of the schema to "infer" that "United States" maps to "USA"

\textbf{Symbolic Fix}: Medium - We may be able to infer the correct condition but we need a way to map that question to the actual values in the database

\subsection{Structural based errors}
These are elements that are a violation of the expected SQL structure (ie clause based errors).

\subsubsection{LIMIT errors}
The LLM doesn't predict a LIMIT at the end of a full query.

\textbf{Symbolic Fix}: Easy - we basically just look at the output and if it matches except the number of rows we just add a LIMIT

\subsubsection{Everything else}

\subsection{Where Conditions}

\subsubsection{Wrong operator}
wrong operator (sometimes the LLM will correctly predict bigger/ greater but not $>$

Symbolic fix: easy for wrong operator - if LLM predicts bigger then just replace it with $>$

upon further inspection there aren't many of these that can be easily fixable (only about 2 were bigger/greater instead of $>$) but I'll leave this category in for now

\subsection{Aggregate Errors}

Wrong aggregate functions like COUNT, MAX, etc

\textbf{Symbolic fix}: hard

\subsection{Other}

catch all

includes syntactically/ semantically "similar" ie they look right....but aren't

\textbf{Symbolic fix}: hard
