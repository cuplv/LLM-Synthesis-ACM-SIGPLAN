\section{Results and Discussion}

We evaluate lexeme-level errors across three LLMs for text-to-SQL synthesis. Errors are categorized according to our proposed taxonomy, and for all experiments, we set the value of k=10. We also track the presence of the gold lexeme and other valid schema elements in the top-10. 

Our results for the overall lexeme-level analysis and categorization are presented in Table ~\ref{tab:error_comparison}. We find that the DeepSeek model struggles more compared to the finetuned models with structural based errors. We also find that the 7B parameter finetuned model performs the best overall with the fewest total number of mistakes. This may indicate the efficacy of finetuning compared to similar sized general coder models. Interestingly, the 7B parameter model also outperforms the 14B parameter finetuned model.

Additionally, we find that our results support our proposed symbolic fixes. We find evidence for the efficacy of our proposed \textit{schema based decoding} method. Across all models, schema based errors persists as a common error type. Across all models, the gold lexeme is found in the top-k between 86-91\% of the time. Of the times it is found in the top-k, it is the highest ranked valid schema element between 50-81\% of the time. This creates strong motivation for the efficacy of such a method. Additionally, we find that LIMIT errors persist across models and comprise of between 19-49\% of Structural based errors suggesting that our proposed post-processing could mitigate many of these errors. The gold lexemes for alias errors, particularly in the finetuned models, occur at high concentrations in the top-k making this error type particularly amenable to constrained decoding. 
