\section{Results}

We evaluate lexeme-level errors across three LLMs for text-to-SQL synthesis. Errors are categorized according to our proposed taxonomy. We also monitor the frequency with which the correct lexeme appears in the top-10 candidates, indicating the potential effectiveness of constrained decoding.
Constrained decoding is likely to be less effective when the correct lexeme does not appear high in the potential candidates since each satisfying, yet incorrect, path would still need to be explored.

Across models, schema grounding remains a constant source of errors, highlighting challenges in correctly selecting tables and columns. Table~\ref{tab:schema_decoding} shows that the correct schema lexeme appears as the highest-ranked valid schema option in 50\%–81\% of errors, suggesting many schema-based errors are recoverable with \textit{schema-constrained decoding}, in which the highest ranked valid schema element is chosen from top-k candidates.

LIMIT errors persist across models as well comprising of between 18-50\% of Structural based errors.


A notable disparity emerges between XiYanSQL-7B and XiYanSQL-14B. While the 14B model reduces structural and operator errors relative to its 7B counterpart (254 vs. 323 structural errors; 126 vs. 165 operator errors), it exhibits a substantial increase in alias errors (415 vs. 27). This suggests that improvements in structural coherence do not uniformly translate to consistent symbolic reference generation. One possible explanation is that the larger model introduces greater variability in alias usage, which is surfaced under lexeme-level incremental validation. Because our evaluation compares each generated lexeme against the canonical gold query, early divergences in alias selection may propagate and increase downstream mismatches. We emphasize that this analysis does not imply semantic incorrectness, but rather highlights representational divergence that becomes visible under incremental evaluation.

Lexeme-level error distributions across models are summarized in Table~\ref{tab:error_comparison}. Structural errors, including misordered clauses and missing LIMITs, remain common, comprising 18–50\% of structural errors. Operator errors, particularly in WHERE and HAVING clauses, also persist across models.

Comparing XiYanSQL-7B and XiYanSQL-14B, we observe that the larger 14B model reduces structural and operator errors (271 vs. 406 and 126 vs. 163, respectively) but exhibits a dramatic increase in alias errors (415 vs. 27). This suggests that improvements in global structural coherence do not uniformly translate to consistent symbolic reference generation. Under lexeme-level evaluation, early divergences in alias predictions propagate, amplifying downstream mismatches, even if the query remains semantically correct at a higher level.

Table~\ref{tab:top_k} presents the fraction of gold lexemes found in the top-10 predictions by category. Schema and aggregation errors often include the correct lexeme in top-k candidates (0.77–0.91), indicating these errors are potentially correctable through constrained decoding. In contrast, many Other / Unclassified errors (0.29–0.68) do not include the gold lexeme in top-k, demonstrating the limits of purely token-level interventions for complex, multi-clause semantic mistakes.

\begin{itemize}
    \item \textbf{Recoverable errors:} A large fraction of schema and aggregation errors could be mitigated with constrained decoding or symbolic repair.
    \item \textbf{Error propagation:} Early divergences in alias or value predictions propagate downstream, highlighting the need for incremental evaluation rather than only final query correctness.
    \item \textbf{Limits of local interventions:} Other / Unclassified errors often require multi-clause reasoning, demonstrating that top-k based interventions alone cannot fully correct complex semantic mistakes.
    \item \textbf{Model scaling effects:} Larger models improve structural coherence but may increase variability in symbolic references, underscoring the importance of lexeme-level explainability for understanding model behavior.
\end{itemize}
