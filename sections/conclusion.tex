\section{Conclusion}
We present a lexeme-level analysis and taxonomy of \emph{incremental errors} in LLM text-to-SQL synthesis. We categorize errors based on common pitfalls and symbolic repairability. Our analysis reveals that 45--78\% of column and table schema grounding errors could be resolvable via schema-constrained decoding. This work establishes clear limits for constrained decoding: success requires correct lexemes in the model's top-k distribution. Easy fixes justify hybrid neuro-symbolic approaches, while Hard errors motivate multi-step refinement frameworks. Our taxonomy provides a structured framework for future text-to-SQL techniques, quantifying the errors LLMs make and suggesting where symbolic methods could complement neural generation.