\section{Conclusion}
We present a lexeme-level analysis and taxonomy for \emph{incremental errors} in LLM text-to-SQL synthesis. We categorize errors based on common pitfalls and symbolic repairability. This work investigates the limits of constrained decoding as success requires correct lexemes to be in the model's top-k distribution. We find that many error categories are amenable to easy symbolic fixes. Our taxonomy provides a structured framework for future text-to-SQL techniques, by quantifying the errors LLMs make at the lexeme level and suggests where symbolic methods could complement neural generation.